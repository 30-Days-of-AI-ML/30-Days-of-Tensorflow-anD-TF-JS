# -*- coding: utf-8 -*-
"""TF-DAY-02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IkBYp_1rBeZ2Fc-kcBWKGGNNwHloC_-d
"""

!pip install tensorflow 
!pip install tensorflow-gpu
#cloud models : AWS /GCP /AZURE : cuda (nvdia)/cudnn

import tensorflow as tf 
import numpy as np 
tf.__version__

#varibles 
#placeholders 
#session - example
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
v1 = tf.Variable(0.0)
p1 = tf.compat.v1.placeholder(tf.float32)
new_val = tf.add(v1,p1)
update = tf.assign(v1 , new_val)

with tf.Session() as sess: 
  sess.run(tf.global_variables_initializer())
  for _ in range(6):
    sess.run(update,feed_dict={p1: 1.0})
    print(sess.run(v1))

#for( i =0 ;i<n ; i++)

#Implementation Linear Regression =Y = mX + c
import numpy as np
import matplotlib.pyplot as plt
#Y = aX + b
X = np.random.rand(100).astype(np.float32) #inputting random values of x upto 100 
a = 50 
b = 40 
Y = a*X  + b
plt.plot(X,Y)

#adding the logic of dependent varible 
Y = np.vectorize(lambda y:y + np.random.normal(loc=0.0 , scale=0.05))(Y)
a_var = tf.Variable(1.0)
b_var = tf.Variable(1.0)
y_var = a_var * X + b_var
#minimizing the error the -- > loss function 
# error in Linear regression : mean squared error 
loss = tf.reduce_mean(tf.square(y_var - Y))
#adam optimiser & gradient optimisers
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)
#Trainning
TRAINING_STEPS= 300
results = []
with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  for step in range(TRAINING_STEPS):
    results.append(sess.run([train , a_var,b_var])[1:])
final_pred = results[-1]
a_hat = final_pred[0]
b_hat = final_pred[0]
y_hat = a_hat*X + b_hat
print("a:", a_hat , "b:",b_hat)

plt.plot(X , Y)
plt.plot(X , y_hat)

# Implementing Logistic Regression by Tensorflow & Comparing with Numpy case
# implement the above code to predict sales ( sales.csv/kaggle)